================================================================================
MICROSCALE LLM COMPRESSION - ANALYSIS SUMMARY
================================================================================

DATASET OVERVIEW
--------------------------------------------------------------------------------
Total experiments: 36
Models tested: gpt2, gpt2-medium, gpt2-large
Compression methods: baseline, pruning, quantization, combined

BASELINE PERFORMANCE
--------------------------------------------------------------------------------
              perplexity  model_size_mb  inference_speed
model                                                   
gpt2         1761.500951     474.700195       198.586300
gpt2-large    508.260174    1476.345215       106.553935
gpt2-medium  1510.849553     676.771484       170.681334

BEST COMPRESSION RESULTS (Lowest Perplexity)
--------------------------------------------------------------------------------

gpt2:
  Compression: combined
  Prune Ratio: 0.1
  Quantization: INT4
  Perplexity: 1425.32
  Model Size: 115.85 MB
  Inference Speed: 72.04 samples/sec

gpt2-medium:
  Compression: quantization
  Quantization: INT8
  Perplexity: 1210.71
  Model Size: 388.77 MB
  Inference Speed: 27.67 samples/sec

gpt2-large:
  Compression: baseline
  Perplexity: 508.26
  Model Size: 1476.35 MB
  Inference Speed: 106.55 samples/sec


COMPRESSION EFFECTIVENESS SUMMARY
--------------------------------------------------------------------------------

GPT2:
  Baseline: Perplexity=1761.50, Size=474.70MB
  Pruning 0.1: Perplexity=1752.07 (-0.5% change)
  Pruning 0.5: Perplexity=1813.85 (+3.0% change)
  Pruning 0.9: Perplexity=80352.67 (+4461.6% change)
  INT8: Perplexity=1576.32 (-10.5%), Size=156.35MB (67.1% reduction)
  INT4: Perplexity=1459.12 (-17.2%), Size=115.85MB (75.6% reduction)

GPT2-MEDIUM:
  Baseline: Perplexity=1510.85, Size=676.77MB
  Pruning 0.1: Perplexity=1516.07 (+0.3% change)
  Pruning 0.5: Perplexity=1393.78 (-7.7% change)
  Pruning 0.9: Perplexity=3791646431.94 (+250961117.4% change)
  INT8: Perplexity=1210.71 (-19.9%), Size=388.77MB (42.6% reduction)
  INT4: Perplexity=1266.06 (-16.2%), Size=244.77MB (63.8% reduction)

GPT2-LARGE:
  Baseline: Perplexity=508.26, Size=1476.35MB
  Pruning 0.1: Perplexity=516.25 (+1.6% change)
  Pruning 0.5: Perplexity=2274.02 (+347.4% change)
  Pruning 0.9: Perplexity=48867.85 (+9514.7% change)
  INT8: Perplexity=1300.59 (+155.9%), Size=801.35MB (45.7% reduction)
  INT4: Perplexity=1201.28 (+136.4%), Size=463.85MB (68.6% reduction)

================================================================================
KEY INSIGHTS:
--------------------------------------------------------------------------------
1. INT4 quantization provides significant model size reduction (~75%)
2. Moderate pruning (10-50%) has minimal impact on perplexity
3. Extreme pruning (90%) can cause catastrophic performance degradation
4. Quantization alone is more stable than aggressive pruning
5. Combined approaches require careful balancing
